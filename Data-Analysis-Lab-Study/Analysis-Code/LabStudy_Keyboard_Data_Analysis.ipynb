{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File for the keyboard data analysis in the lab-study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "plt.style.use(\"seaborn-deep\")\n",
    "\n",
    "# SK Learn imports\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import permutation_test_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Imports for SK Learn RepeatedGroupKFold and Permutation Test Customizations\n",
    "# imports to copy sk learn functionality and adapt it to the needs of our data\n",
    "import numbers\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import warnings\n",
    "from inspect import signature\n",
    "from sklearn.utils.validation import _num_samples, _deprecate_positional_args, check_array\n",
    "from sklearn.base import _pprint\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection._split import check_cv\n",
    "from sklearn.base import is_classifier, clone\n",
    "from sklearn.utils import (indexable, check_random_state, _safe_indexing)\n",
    "from sklearn.metrics import check_scoring\n",
    "from sklearn.utils.metaestimators import _safe_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the keyboard feature dataset as a pandas dataframe (the dataset is in long format)\n",
    "keyboard_data_long = pd.read_csv(\"Labstudy_Keyboard_Features.csv\", sep=\"\\t\", encoding=\"utf-8\", index_col=0)\n",
    "\n",
    "# import the manipulation check variables dataset to complement the keyboard dataset\n",
    "man_check_variables_data = pd.read_csv(\"Lab_Study_Manipulation_Check_Variables_for_ML.csv\", sep=\"\\t\", encoding=\"utf-8\",\n",
    "                                       index_col=0)\n",
    "\n",
    "# merge both datasets together into one dataset\n",
    "keyboard_data_long = pd.concat([keyboard_data_long, man_check_variables_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a dataframe in wide format (for dependent t-tests and descriptive statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get a name of the relevant columns for the wide data format\n",
    "keyboard_column_names = [i for i in keyboard_data_long.columns if i != \"par_Num\" and i != \"condition\"\n",
    "                         and i != \"BPM\" and i != \"EDA\" and i != \"Valence\" and i != \"Arousal\"]\n",
    "\n",
    "# change the data from long to wide format\n",
    "keyboard_data_wide = keyboard_data_long.pivot_table(index=['par_Num'], columns='condition', values=keyboard_column_names,\n",
    "                                                    aggfunc='first')\n",
    "\n",
    "# assign column names\n",
    "keyboard_data_wide.columns = [f'{x}_{y}' for x, y in keyboard_data_wide.columns]\n",
    "\n",
    "\n",
    "print(\"Shape of long format dataset:\", keyboard_data_long.shape,\n",
    "      \"Shape of wide format dataset:\", keyboard_data_wide.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Compare the Keyboard Typing Features between the low-stress and high-stress condition ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 Plot each variable pair (the same variable from the high-stress and low-stress condition) in one plot\n",
    "for col in keyboard_data_wide.columns:\n",
    "    if \"HS\" in col:\n",
    "        # find the corresponding low_stress variable\n",
    "        ls_string = col.replace(\"HS\", \"LS\")\n",
    "        # create a neutral variable name\n",
    "        neutral_string = col.replace(\"HS\", \"\")\n",
    "        # plot both variables in one plot\n",
    "        # side by side\n",
    "        # plt.hist([keyboard_data_wide[col], keyboard_data_wide[ls_string]], label=[\"HS\", \"LS\"])\n",
    "        # on top of each other\n",
    "        # plt.hist(keyboard_data_wide[col], alpha=0.5, label=\"HS\")\n",
    "        # plt.hist(keyboard_data_wide[ls_string], alpha=0.5, label=\"LS\")\n",
    "        # as seaborn distplots\n",
    "        sns.distplot(keyboard_data_wide[col], hist=True, kde=True, kde_kws={\"linewidth\": 3}, label=\"HS\")\n",
    "        sns.distplot(keyboard_data_wide[ls_string], hist=True, kde=True, kde_kws={\"linewidth\": 3}, label=\"LS\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(neutral_string)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Get some descriptive stats about the datatset\n",
    "desc_stats_wide_data = keyboard_data_wide.describe().sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitialize an empty dataframe to store the results of the paired sample t-test for each variable\n",
    "paired_ttest_results = pd.DataFrame()\n",
    "\n",
    "# 3 Run paired-sample T-Tests to compare each variable\n",
    "for col in keyboard_data_wide.columns:\n",
    "    if \"HS\" in col:\n",
    "        # find the corresponding low_stress variable\n",
    "        ls_string = col.replace(\"HS\", \"LS\")\n",
    "        # create a neutral variable name\n",
    "        neutral_string = col.replace(\"HS\", \"\")[:-1]\n",
    "        # calculate the paired sample t-test\n",
    "        paired_ttest = pg.ttest(keyboard_data_wide[col], keyboard_data_wide[ls_string], paired=True)\n",
    "        # rename the pingouin index of the t-test dataframe to the variable name\n",
    "        paired_ttest.index = [neutral_string]\n",
    "        # concat the results of the variables t-test to the result dataframe\n",
    "        paired_ttest_results = pd.concat([paired_ttest_results, paired_ttest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Data Plotting before machine learning analysis ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Plot each Variable in a distplot\n",
    "for col in keyboard_data_long.drop([\"condition\", \"par_Num\"], axis=1).columns:\n",
    "    sns.distplot(keyboard_data_long[col], hist=True, kde=True, hist_kws={'edgecolor':'black'},\n",
    "                 kde_kws={\"linewidth\": 3})\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Plot a pairplot with all relevant variables\n",
    "sns.pairplot(keyboard_data_long.drop([\"condition\", \"par_Num\"], axis=1), corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Plot a correlation matrix between the features\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = keyboard_data_long.drop([\"condition\", \"par_Num\"], axis=1).corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5,\n",
    "            cbar_kws={\"shrink\": .5}, annot=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --Machine learning analysis--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Customize SK Learn code to implement repeated-group-k-fold cross validation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Created a custom Repeated-Group-K-Fold Splitter that can be included in the Scikit-Learn Pipeline ---\n",
    "\n",
    "# --- Copied from the Scikit Learn model_selection/_split.py\n",
    "def _build_repr(self):\n",
    "    # XXX This is copied from BaseEstimator's get_params\n",
    "    cls = self.__class__\n",
    "    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n",
    "    # Ignore varargs, kw and default values and pop self\n",
    "    init_signature = signature(init)\n",
    "    # Consider the constructor parameters excluding 'self'\n",
    "    if init is object.__init__:\n",
    "        args = []\n",
    "    else:\n",
    "        args = sorted([p.name for p in init_signature.parameters.values()\n",
    "                       if p.name != 'self' and p.kind != p.VAR_KEYWORD])\n",
    "    class_name = self.__class__.__name__\n",
    "    params = dict()\n",
    "    for key in args:\n",
    "        # We need deprecation warnings to always be on in order to\n",
    "        # catch deprecated param values.\n",
    "        # This is set in utils/__init__.py but it gets overwritten\n",
    "        # when running under python3 somehow.\n",
    "        warnings.simplefilter(\"always\", FutureWarning)\n",
    "        try:\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                value = getattr(self, key, None)\n",
    "                if value is None and hasattr(self, 'cvargs'):\n",
    "                    value = self.cvargs.get(key, None)\n",
    "            if len(w) and w[0].category == FutureWarning:\n",
    "                # if the parameter is deprecated, don't show it\n",
    "                continue\n",
    "        finally:\n",
    "            warnings.filters.pop(0)\n",
    "        params[key] = value\n",
    "\n",
    "    return '%s(%s)' % (class_name, _pprint(params, offset=len(class_name)))\n",
    "\n",
    "\n",
    "class BaseCrossValidator(metaclass=ABCMeta):\n",
    "    \"\"\"Base class for all cross-validators\n",
    "    Implementations must define `_iter_test_masks` or `_iter_test_indices`.\n",
    "    \"\"\"\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target variable for supervised learning problems.\n",
    "        groups : array-like of shape (n_samples,), default=None\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        indices = np.arange(_num_samples(X))\n",
    "        for test_index in self._iter_test_masks(X, y, groups):\n",
    "            train_index = indices[np.logical_not(test_index)]\n",
    "            test_index = indices[test_index]\n",
    "            yield train_index, test_index\n",
    "\n",
    "    # Since subclasses must implement either _iter_test_masks or\n",
    "    # _iter_test_indices, neither can be abstract.\n",
    "    def _iter_test_masks(self, X=None, y=None, groups=None):\n",
    "        \"\"\"Generates boolean masks corresponding to test sets.\n",
    "        By default, delegates to _iter_test_indices(X, y, groups)\n",
    "        \"\"\"\n",
    "        for test_index in self._iter_test_indices(X, y, groups):\n",
    "            test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
    "            test_mask[test_index] = True\n",
    "            yield test_mask\n",
    "\n",
    "    def _iter_test_indices(self, X=None, y=None, groups=None):\n",
    "        \"\"\"Generates integer indices corresponding to test sets.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return _build_repr(self)\n",
    "\n",
    "\n",
    "class _BaseKFold(BaseCrossValidator, metaclass=ABCMeta):\n",
    "    \"\"\"Base class for KFold, GroupKFold, and StratifiedKFold\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, n_splits, *, shuffle, random_state):\n",
    "        if not isinstance(n_splits, numbers.Integral):\n",
    "            raise ValueError('The number of folds must be of Integral type. '\n",
    "                             '%s of type %s was passed.'\n",
    "                             % (n_splits, type(n_splits)))\n",
    "        n_splits = int(n_splits)\n",
    "\n",
    "        if n_splits <= 1:\n",
    "            raise ValueError(\n",
    "                \"k-fold cross-validation requires at least one\"\n",
    "                \" train/test split by setting n_splits=2 or more,\"\n",
    "                \" got n_splits={0}.\".format(n_splits))\n",
    "\n",
    "        if not isinstance(shuffle, bool):\n",
    "            raise TypeError(\"shuffle must be True or False;\"\n",
    "                            \" got {0}\".format(shuffle))\n",
    "\n",
    "        if not shuffle and random_state is not None:  # None is the default\n",
    "            # TODO 0.24: raise a ValueError instead of a warning\n",
    "            warnings.warn(\n",
    "                'Setting a random_state has no effect since shuffle is '\n",
    "                'False. This will raise an error in 0.24. You should leave '\n",
    "                'random_state to its default (None), or set shuffle=True.',\n",
    "                FutureWarning\n",
    "            )\n",
    "\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,), default=None\n",
    "            The target variable for supervised learning problems.\n",
    "        groups : array-like of shape (n_samples,), default=None\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        if self.n_splits > n_samples:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of splits n_splits={0} greater\"\n",
    "                 \" than the number of samples: n_samples={1}.\")\n",
    "                .format(self.n_splits, n_samples))\n",
    "\n",
    "        for train, test in super().split(X, y, groups):\n",
    "            yield train, test\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        y : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        n_splits : int\n",
    "            Returns the number of splitting iterations in the cross-validator.\n",
    "        \"\"\"\n",
    "        return self.n_splits\n",
    "\n",
    "\n",
    "class _RepeatedSplits(metaclass=ABCMeta):\n",
    "    \"\"\"Repeated splits for an arbitrary randomized CV splitter.\n",
    "    Repeats splits for cross-validators n times with different randomization\n",
    "    in each repetition.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv : callable\n",
    "        Cross-validator class.\n",
    "    n_repeats : int, default=10\n",
    "        Number of times cross-validator needs to be repeated.\n",
    "    random_state : int or RandomState instance, default=None\n",
    "        Passes `random_state` to the arbitrary repeating cross validator.\n",
    "        Pass an int for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    **cvargs : additional params\n",
    "        Constructor parameters for cv. Must not contain random_state\n",
    "        and shuffle.\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, cv, *, n_repeats=10, random_state=None, **cvargs):\n",
    "        if not isinstance(n_repeats, numbers.Integral):\n",
    "            raise ValueError(\"Number of repetitions must be of Integral type.\")\n",
    "\n",
    "        if n_repeats <= 0:\n",
    "            raise ValueError(\"Number of repetitions must be greater than 0.\")\n",
    "\n",
    "        if any(key in cvargs for key in ('random_state', 'shuffle')):\n",
    "            raise ValueError(\n",
    "                \"cvargs must not contain random_state or shuffle.\")\n",
    "\n",
    "        self.cv = cv\n",
    "        self.n_repeats = n_repeats\n",
    "        self.random_state = random_state\n",
    "        self.cvargs = cvargs\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generates indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of length n_samples\n",
    "            The target variable for supervised learning problems.\n",
    "        groups : array-like of shape (n_samples,), default=None\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        n_repeats = self.n_repeats\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        for idx in range(n_repeats):\n",
    "            cv = self.cv(random_state=rng, shuffle=True,\n",
    "                         **self.cvargs)\n",
    "            for train_index, test_index in cv.split(X, y, groups):\n",
    "                yield train_index, test_index\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : object\n",
    "            Always ignored, exists for compatibility.\n",
    "            ``np.zeros(n_samples)`` may be used as a placeholder.\n",
    "        y : object\n",
    "            Always ignored, exists for compatibility.\n",
    "            ``np.zeros(n_samples)`` may be used as a placeholder.\n",
    "        groups : array-like of shape (n_samples,), default=None\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Returns\n",
    "        -------\n",
    "        n_splits : int\n",
    "            Returns the number of splitting iterations in the cross-validator.\n",
    "        \"\"\"\n",
    "        rng = check_random_state(self.random_state)\n",
    "        cv = self.cv(random_state=rng, shuffle=True,\n",
    "                     **self.cvargs)\n",
    "        return cv.get_n_splits(X, y, groups) * self.n_repeats\n",
    "\n",
    "    def __repr__(self):\n",
    "        return _build_repr(self)\n",
    "\n",
    "\n",
    "# --- Adapted the GroupKFold class to be able to randomly shuffle the groups for repeated-group-k-fold-cross-validation\n",
    "# The group indices are randomly shuffled and then split into train and test datasets -> The sk learn GroupKFold\n",
    "# class does not allow random shuffling, because it splits based on the group sizes (which are equal in my dataset) ---\n",
    "class RandomGroupKFold(_BaseKFold):\n",
    "\n",
    "    def __init__(self, n_splits=5, *, shuffle=False,\n",
    "                 random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def _iter_test_indices(self, X, y, groups):\n",
    "        if groups is None:\n",
    "            raise ValueError(\"The 'groups' parameter should not be None.\")\n",
    "        groups = check_array(groups, ensure_2d=False, dtype=None)\n",
    "\n",
    "        unique_groups, groups = np.unique(groups, return_inverse=True)\n",
    "        n_groups = len(unique_groups)\n",
    "\n",
    "        if self.n_splits > n_groups:\n",
    "            raise ValueError(\"Cannot have number of splits n_splits=%d greater\"\n",
    "                             \" than the number of groups: %d.\"\n",
    "                             % (self.n_splits, n_groups))\n",
    "\n",
    "        # Weight groups by their number of occurrences\n",
    "        n_samples_per_group = np.bincount(groups)\n",
    "\n",
    "        # --- The code below is apated to randomly split the groups instead of splitting the groups based on group size\n",
    "        # Distribute the most frequent groups first\n",
    "        indices = np.arange(n_groups)\n",
    "\n",
    "        if self.shuffle:\n",
    "            check_random_state(self.random_state).shuffle(indices)\n",
    "\n",
    "        # --- End of adapted code: Below is the original code from the sk learn GroupKFold class ---\n",
    "\n",
    "        n_samples_per_group = n_samples_per_group[indices]\n",
    "\n",
    "        # Total weight of each fold\n",
    "        n_samples_per_fold = np.zeros(self.n_splits)\n",
    "\n",
    "        # Mapping from group index to fold index\n",
    "        group_to_fold = np.zeros(len(unique_groups))\n",
    "\n",
    "        # Distribute samples by adding the largest weight to the lightest fold\n",
    "        for group_index, weight in enumerate(n_samples_per_group):\n",
    "            lightest_fold = np.argmin(n_samples_per_fold)\n",
    "            n_samples_per_fold[lightest_fold] += weight\n",
    "            group_to_fold[indices[group_index]] = lightest_fold\n",
    "\n",
    "        indices = group_to_fold[groups]\n",
    "\n",
    "        for f in range(self.n_splits):\n",
    "            yield np.where(indices == f)[0]\n",
    "\n",
    "\n",
    "# Added the class to be able to do Repeated GroupKFold Splitting: Copied and adapted the RepeatedKFold Class\n",
    "class RepeatedGroupKFold(_RepeatedSplits):\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, *, n_splits=5, n_repeats=10, random_state=None):\n",
    "        super().__init__(\n",
    "            RandomGroupKFold, n_repeats=n_repeats,\n",
    "            random_state=random_state, n_splits=n_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Customize the sk learn permutation_test_score function ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Customize the SK Learn permutation_test_score function to return the list of all scores during cross validation\n",
    "# instead of the mean of the list (as it is implemented in sk learn): The files are copy and pasted from the\n",
    "# SK Learn model_selection/_validation.py file---\n",
    "\n",
    "# SK Learn permutation Test score function adopted to the repeated group characteristics\n",
    "@_deprecate_positional_args\n",
    "def rep_group_permutation_test_score(estimator, X, y, *, groups=None, cv=None,\n",
    "                           n_permutations=100, n_jobs=None, random_state=0,\n",
    "                           verbose=0, scoring=None):\n",
    "    \"\"\"Evaluate the significance of a cross-validated score with permutations\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    groups : array-like of shape (n_samples,), default=None\n",
    "        Labels to constrain permutation within groups, i.e. ``y`` values\n",
    "        are permuted among samples with the same group identifier.\n",
    "        When not specified, ``y`` values are permuted among all samples.\n",
    "        When a grouped cross-validator is used, the group labels are\n",
    "        also passed on to the ``split`` method of the cross-validator. The\n",
    "        cross-validator uses them for grouping the samples  while splitting\n",
    "        the dataset into train/test set.\n",
    "    scoring : str or callable, default=None\n",
    "        A single str (see :ref:`scoring_parameter`) or a callable\n",
    "        (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
    "        If None the estimator's score method is used.\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "        - None, to use the default 5-fold cross validation,\n",
    "        - int, to specify the number of folds in a `(Stratified)KFold`,\n",
    "        - :term:`CV splitter`,\n",
    "        - An iterable yielding (train, test) splits as arrays of indices.\n",
    "        For int/None inputs, if the estimator is a classifier and ``y`` is\n",
    "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
    "        other cases, :class:`KFold` is used.\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validation strategies that can be used here.\n",
    "        .. versionchanged:: 0.22\n",
    "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
    "    n_permutations : int, default=100\n",
    "        Number of times to permute ``y``.\n",
    "    n_jobs : int, default=None\n",
    "        The number of CPUs to use to do the computation.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "    random_state : int, RandomState instance or None, default=0\n",
    "        Pass an int for reproducible output for permutation of\n",
    "        ``y`` values among samples. See :term:`Glossary <random_state>`.\n",
    "    verbose : int, default=0\n",
    "        The verbosity level.\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        The true score without permuting targets.\n",
    "    permutation_scores : array of shape (n_permutations,)\n",
    "        The scores obtained for each permutations.\n",
    "    pvalue : float\n",
    "        The p-value, which approximates the probability that the score would\n",
    "        be obtained by chance. This is calculated as:\n",
    "        `(C + 1) / (n_permutations + 1)`\n",
    "        Where C is the number of permutations whose score >= the true score.\n",
    "        The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.\n",
    "    Notes\n",
    "    -----\n",
    "    This function implements Test 1 in:\n",
    "        Ojala and Garriga. Permutation Tests for Studying Classifier\n",
    "        Performance.  The Journal of Machine Learning Research (2010)\n",
    "        vol. 11\n",
    "        `[pdf] <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_.\n",
    "    \"\"\"\n",
    "    X, y, groups = indexable(X, y, groups)\n",
    "\n",
    "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
    "    scorer = check_scoring(estimator, scoring=scoring)\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    # We clone the estimator to make sure that all the folds are\n",
    "    # independent, and that it is pickle-able.\n",
    "    # returned scores are list of scores instead of the mean score\n",
    "    scores = _permutation_test_score(clone(estimator), X, y, groups, cv, scorer)\n",
    "    permutation_scores = Parallel(n_jobs=n_jobs, verbose=verbose)(\n",
    "        delayed(_permutation_test_score)(\n",
    "            clone(estimator), X, _shuffle(y, groups, random_state),\n",
    "            groups, cv, scorer)\n",
    "        for _ in range(n_permutations))\n",
    "    #  in repeated 5-fold cross validation, it is a 25 * n_permutations matrix --> To get the get \"original\" permutation\n",
    "    # p-value, we need to take the mean on axis 1 to get mean(25 cv scores) * n_permutations\n",
    "    permutation_scores = np.array(permutation_scores)\n",
    "    pvalue = (np.sum(np.mean(permutation_scores, axis=1) >= np.mean(scores)) + 1.0) / (n_permutations + 1)\n",
    "    return scores, permutation_scores, pvalue\n",
    "\n",
    "\n",
    "# Changed this function to return the list of scores instead of the mean of the list of scores\n",
    "def _permutation_test_score(estimator, X, y, groups, cv, scorer):\n",
    "    \"\"\"Auxiliary function for permutation_test_score\"\"\"\n",
    "    scores = []\n",
    "    for train, test in cv.split(X, y, groups):\n",
    "        X_train, y_train = _safe_split(estimator, X, y, train)\n",
    "        X_test, y_test = _safe_split(estimator, X, y, test, train)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        scores.append(scorer(estimator, X_test, y_test))\n",
    "    return scores\n",
    "\n",
    "\n",
    "# this function is unchanged\n",
    "def _shuffle(y, groups, random_state):\n",
    "    \"\"\"Return a shuffled copy of y eventually shuffle among same groups.\"\"\"\n",
    "    if groups is None:\n",
    "        indices = random_state.permutation(len(y))\n",
    "    else:\n",
    "        indices = np.arange(len(groups))\n",
    "        for group in np.unique(groups):\n",
    "            this_mask = (groups == group)\n",
    "            indices[this_mask] = random_state.permutation(indices[this_mask])\n",
    "    return _safe_indexing(y, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Helper Function to carry out the machine learning analysis ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the results of the permutation procedure\n",
    "def plot_permutation_test_results(permutation_scores, model_scores, pvalue, p_tresh, method, plot_title):\n",
    "\n",
    "    # set the default colors to the seaborn color codes\n",
    "    sns.set_color_codes()\n",
    "\n",
    "    # initialize a figure with two subplots (one above the other with a 15%, 85% ratio)\n",
    "    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n",
    "\n",
    "    # plot a boxplot of the\n",
    "    sns.boxplot(model_scores, color=\"LightYellow\", showmeans=True, meanline=True,\n",
    "                meanprops={\"linestyle\": \"-\", \"linewidth\": \"3\", \"color\": \"DarkKhaki\"},\n",
    "                medianprops={\"linestyle\": \"None\", \"linewidth\": \"0\"},\n",
    "                flierprops={\"marker\": \"o\", \"markerfacecolor\": \"LightYellow\"},\n",
    "                ax=ax_box)\n",
    "\n",
    "    # set the x_label of the Boxplot to None\n",
    "    ax_box.set_xlabel(\"\")\n",
    "    # Remove the borders of the \"graph\" around the boxplot\n",
    "    ax_box.axis(\"off\")\n",
    "\n",
    "    # plot a histogram of the permutation scores\n",
    "    sns.histplot(permutation_scores, color=\"b\", bins=15, ax=ax_hist, label=\"Permutation Scores\")\n",
    "\n",
    "    # plot a vertical line of the mean cv score\n",
    "    ax_hist.axvline(np.mean(model_scores), color=\"DarkKhaki\", linewidth=3,\n",
    "                    label=\"Model Score: %s (%s), \\np = %s\" % (\n",
    "                    np.round(np.mean(model_scores), 2), np.round(np.std(model_scores), 2), pvalue))\n",
    "    # plot a vertical line of the mean permutation score\n",
    "    ax_hist.axvline(np.mean(permutation_scores), color=\"Navy\", linewidth=3,\n",
    "               label=\"Permutation Mean: \" \"%s\" % (np.round(np.mean(permutation_scores), 2)))\n",
    "    # plot a vertical line of the significance threshold\n",
    "    ax_hist.axvline(p_tresh,\n",
    "             label=\"Sig. Threshold: \" \"%s\" % p_tresh, color=\"darkgreen\", linestyle=\"--\", linewidth=3)\n",
    "\n",
    "    # remove the top and left corner of the\n",
    "    sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "\n",
    "    # set title\n",
    "    ax_box.set(title=plot_title)\n",
    "    # set the axis labels (depends on the score used for classification or regression)\n",
    "    if method == \"Classification\":\n",
    "        score_label = \"Accuracy Score\"\n",
    "    else:\n",
    "        score_label = \"R² Score\"\n",
    "    plt.xlabel(score_label)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    # loc legend to the top left\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(plot_title + \".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# calculates a k-fold cross val score and compares the mean performance score with a distribution of n models that\n",
    "# were trained with permutated class labels\n",
    "# more information can be found here:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html\n",
    "def ml_permutation_test(iv, dv, algorithm, groups, method, cv_repeats, permutation_repeats, procedure_title):\n",
    "\n",
    "    # save the results of the permutation test\n",
    "    results = {}\n",
    "\n",
    "    # initiate the pipeline and select the desired configurations for handling multicollinearity and\n",
    "    # the feature selection procedure\n",
    "    # make a pipeline that does the preprocessing and outputs the cross validation score\n",
    "    pipeline = Pipeline([\n",
    "        (\"standardization\", StandardScaler()),\n",
    "        (\"clf\", algorithm)\n",
    "    ])\n",
    "\n",
    "    # initialize the repeated k-fold iterator with non-overlapping groups\n",
    "    cv = RepeatedGroupKFold(n_splits=5, n_repeats=cv_repeats)\n",
    "    # cv = GroupKFold(n_splits=5)\n",
    "\n",
    "    # get the scoring function\n",
    "    if method == \"Classification\":\n",
    "        scorer = \"accuracy\"\n",
    "    else:\n",
    "        scorer = \"r2\"\n",
    "\n",
    "    # get the cross validation scores, the standard deviation of the cv scores, the permuation scores and the\n",
    "    # p-value of the permutation test\n",
    "    scores, permutation_scores, pvalue = rep_group_permutation_test_score(\n",
    "        pipeline, iv, dv, scoring=scorer, cv=cv, groups=groups, n_permutations=permutation_repeats)\n",
    "\n",
    "    # get the statistical values of the model scores\n",
    "    model_score = np.mean(scores)\n",
    "    model_std = np.std(scores)\n",
    "    model_std_err = sem(scores)\n",
    "\n",
    "    # get the statistical values of the permutation (in repeated cv, its n_splits * n_repeats scores for each\n",
    "    # permutation run --> num of permutation_scores = n_splits * n_repeats scores * n_permutations)\n",
    "    perm_score = np.mean(permutation_scores)\n",
    "    perm_std = np.std(np.mean(permutation_scores, axis=1))\n",
    "    # Get the significance threshold (the classification model must be better than 95% of the permutated models)\n",
    "    sig_tresh = np.percentile(np.mean(permutation_scores, axis=1), 95.0)\n",
    "\n",
    "    # save the results\n",
    "    results[\"Mean_score\"] = np.round(model_score, 2)\n",
    "    results[\"SD_score\"] = np.round(model_std, 2)\n",
    "    results[\"SE_score\"] = np.round(model_std_err, 2)\n",
    "    results[\"p_value\"] = np.round(pvalue, 5)\n",
    "    results[\"Mean_Permutation_score\"] = np.round(perm_score, 2)\n",
    "    results[\"Std_Permutation_score\"] = np.round(perm_std, 2)\n",
    "    results[\"Sig_Treshold\"] = sig_tresh\n",
    "\n",
    "    plot_permutation_test_results(permutation_scores=np.mean(permutation_scores, axis=1),\n",
    "                                  model_scores=scores,\n",
    "                                  pvalue=np.round(pvalue, 3),\n",
    "                                  p_tresh=np.round(sig_tresh, 2),\n",
    "                                  method=method,\n",
    "                                  plot_title=procedure_title)\n",
    "\n",
    "    # output the results and scores\n",
    "    return results, scores\n",
    "\n",
    "\n",
    "# helper function for repeated k-fold cross validation without a permutation test\n",
    "def rep_kfold_cv(iv, dv, algorithm, groups, method, cv_repeats):\n",
    "\n",
    "    # save the results of the permutation test\n",
    "    results = {}\n",
    "\n",
    "    # initiate the pipeline and select the desired configurations for handling multicollinearity and\n",
    "    # the feature selection procedure\n",
    "    # make a pipeline that does the preprocessing and outputs the cross validation score\n",
    "    pipeline = Pipeline([\n",
    "        (\"standardization\", StandardScaler()),\n",
    "        (\"clf\", algorithm)\n",
    "    ])\n",
    "\n",
    "    # initialize the repeated k-fold iterator with non-overlapping groups\n",
    "    cv = RepeatedGroupKFold(n_splits=5, n_repeats=cv_repeats)\n",
    "\n",
    "    # get the scoring function\n",
    "    if method == \"Classification\":\n",
    "        scorer = \"accuracy\"\n",
    "    else:\n",
    "        scorer = \"r2\"\n",
    "\n",
    "    # get the scores of repeated 5-fold cross validation\n",
    "    scores = cross_val_score(pipeline, X=iv, y=dv, groups=groups, scoring=scorer, cv=cv)\n",
    "\n",
    "    # calculate and save the results\n",
    "    results[\"Mean_score\"] = np.round(np.mean(scores), 2)\n",
    "    results[\"SD_score\"] = np.round(np.std(scores), 2)\n",
    "    results[\"SE_score\"] = np.round(sem(scores), 2)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Helper function to run the machine learning analysis with the selected settings ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the permutation test with the \"final settings\" and save the results in a pandas dataframe\n",
    "def get_ml_results(dataset, method, permutation_test, dv):\n",
    "\n",
    "    # get a list of all keyboard features (machine learning predictors)\n",
    "    keyboard_features = [i for i in dataset.columns if i != \"par_Num\" and i != \"condition\"\n",
    "                         and i != \"BPM\" and i != \"EDA\" and i != \"Valence\" and i != \"Arousal\"]\n",
    "\n",
    "    ml_results = {}\n",
    "\n",
    "    # dict with different algorithms for classification/regression: add or remove algorithms here\n",
    "    # we chose a simple neighbors classifier and a lasso regression with cv search for the best alpha\n",
    "    algorithms = {\n",
    "        \"Classification\": {\n",
    "            \"kNN_class\": KNeighborsClassifier(n_neighbors=3)},\n",
    "        \"Regression\": {\n",
    "            \"RidgeReg\": RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])}\n",
    "\n",
    "    }\n",
    "\n",
    "    # drop rows if the column of the dependent variable contains NaN\n",
    "    dataset = dataset.dropna(subset=[dv])\n",
    "\n",
    "    # get the dependent and independent variable of the machine learning analysis\n",
    "    predictors = dataset.loc[:, keyboard_features]\n",
    "    dependent_variable = dataset[dv]\n",
    "    if method == \"Classification\":\n",
    "        dependent_variable = dependent_variable.replace([\"HS\", \"LS\"], [0., 1.])\n",
    "\n",
    "    # perform the permuation test with all algorithms\n",
    "    for alg in algorithms[method]:\n",
    "        if permutation_test:\n",
    "            analysis_title = method + \" Permutation test with target \" + dv + \" using \" + alg\n",
    "            print(analysis_title)\n",
    "            # get the results of the permutation test\n",
    "            result_dic, model_scores = ml_permutation_test(iv=predictors,\n",
    "                                                           dv=dependent_variable,\n",
    "                                                           algorithm=algorithms[method][alg],\n",
    "                                                           groups=dataset[\"par_Num\"],\n",
    "                                                           method=method,\n",
    "                                                           cv_repeats=5,\n",
    "                                                           permutation_repeats=1000,\n",
    "                                                           procedure_title=analysis_title)\n",
    "        else:\n",
    "            analysis_title = method + \" Repeated cv with target \" + dv + \" using \" + alg\n",
    "            print(analysis_title)\n",
    "            result_dic = rep_kfold_cv(iv=predictors,\n",
    "                                      dv=dependent_variable,\n",
    "                                      algorithm=algorithms[method][alg],\n",
    "                                      groups=dataset[\"par_Num\"],\n",
    "                                      method=method,\n",
    "                                      cv_repeats=5)\n",
    "            print(result_dic)\n",
    "\n",
    "        # save the results with labeled index in the results dictionary\n",
    "        ml_results[(method, dv, alg)] = result_dic\n",
    "\n",
    "    # create a multiindexed dataframe from the results dictionary\n",
    "    permutation_results_df = pd.DataFrame(ml_results).T\n",
    "\n",
    "    return permutation_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Do the machine learning analysis ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the raw keyboard feature data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 Condition Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 Condition Classification\n",
    "cond_classification_results = get_ml_results(keyboard_data_long, method=\"Classification\", dv=\"condition\",\n",
    "                                             permutation_test=True)\n",
    "\n",
    "# add an index key to the results dataframe\n",
    "cond_classification_results = pd.concat([cond_classification_results], keys=[\"Raw_Data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2 Regression on Valence, Arousal, BPM and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valence\n",
    "valence_regression_results = get_ml_results(keyboard_data_long, method=\"Regression\", dv=\"Valence\",\n",
    "                                            permutation_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPM\n",
    "bpm_regression_results = get_ml_results(keyboard_data_long, method=\"Regression\", dv=\"BPM\",\n",
    "                                        permutation_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "eda_regression_results = get_ml_results(keyboard_data_long, method=\"Regression\", dv=\"EDA\",\n",
    "                                        permutation_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the regression results into one dataframe and add a raw data index\n",
    "merged_results = pd.concat([pd.concat([valence_regression_results, arousal_regression_results,\n",
    "                            bpm_regression_results, eda_regression_results])], keys=[\"Raw_Data\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- create a difference score dataset to do the same analysis with the difference score data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 Create difference score dataset\n",
    "def create_difference_score_data(dataset):\n",
    "\n",
    "    # calculate the difference scores (substract rows from each other)\n",
    "    diff_part1 = dataset.drop(\"condition\", axis=1).groupby(\"par_Num\").diff()\n",
    "    diff_part2 = dataset.drop(\"condition\", axis=1).groupby(\"par_Num\").diff(periods=-1)\n",
    "    # \"Merge both dataframes together to replace the missing values with the difference scores\n",
    "    merged_diff = diff_part1.fillna(diff_part2)\n",
    "    # add the remaining variables to the dataframe\n",
    "    merged_diff = pd.concat([merged_diff, dataset.loc[:, [\"condition\", \"par_Num\"]]], axis=1)\n",
    "    print(diff_part1.shape, diff_part2.shape, merged_diff.shape)\n",
    "\n",
    "    return merged_diff\n",
    "\n",
    "\n",
    "diff_dataset = create_difference_score_data(keyboard_data_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condition Classification using the difference score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cond_classification_results = get_ml_results(diff_dataset, method=\"Classification\", dv=\"condition\",\n",
    "                                                  permutation_test=True)\n",
    "\n",
    "# add a raw data index to the results dataframe\n",
    "diff_cond_classification_results = pd.concat([diff_cond_classification_results], keys=[\"Diff_Data\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression on valence, arousal. BPM and EDA using the difference score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valence\n",
    "diff_valence_regression_results = get_ml_results(diff_dataset, method=\"Regression\", dv=\"Valence\",\n",
    "                                                 permutation_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arousal\n",
    "diff_arousal_regression_results = get_ml_results(diff_dataset, method=\"Regression\", dv=\"Arousal\",\n",
    "                                                 permutation_test=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPM\n",
    "diff_bpm_regression_results = get_ml_results(diff_dataset, method=\"Regression\", dv=\"BPM\",\n",
    "                                             permutation_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the regression results into one dataframe and add a raw data index\n",
    "diff_merged_results = pd.concat([pd.concat([diff_valence_regression_results,\n",
    "                                 diff_arousal_regression_results, diff_bpm_regression_results,\n",
    "                                 diff_eda_regression_results])], keys=[\"Diff_Data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Save the results of all data analysis in one excel file ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the results and save all data analysis results in one excel file\n",
    "ml_regression_results = pd.concat([merged_results, diff_merged_results])\n",
    "ml_classification_results = pd.concat([cond_classification_results, diff_cond_classification_results])\n",
    "\n",
    "# save the dataframe plus the descriptive stats as an excel file\n",
    "with pd.ExcelWriter(\"Labstudy_Keyboard_Feature_Analysis_Results.xlsx\") as writer:\n",
    "    desc_stats_wide_data.to_excel(writer, float_format=\"%.3f\", sheet_name=\"Descriptive_Stats\")\n",
    "    paired_ttest_results.to_excel(writer, float_format=\"%.4f\", sheet_name=\"t-test_results\")\n",
    "    ml_classification_results.to_excel(writer, float_format=\"%.4f\", sheet_name=\"Machine_Learning_Classification\")\n",
    "    ml_regression_results.to_excel(writer, float_format=\"%.4f\", sheet_name=\"Machine_Learning_regression\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:full_conda]",
   "language": "python",
   "name": "conda-env-full_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
