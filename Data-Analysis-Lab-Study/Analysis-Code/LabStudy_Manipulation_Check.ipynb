{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File to perform the manipulation check in the lab-study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import pandas as pd\n",
    "import operator\n",
    "from typing import List\n",
    "import hrvanalysis\n",
    "import pingouin as pg\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the lab_study firebase dataset\n",
    "with open(\"LabStudy_RawData.json\") as f:\n",
    "    firebase_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for physiological data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the phase timestamps to be able to calculate the physiological parameters for the phase\n",
    "def get_phase_timestamps(data):\n",
    "\n",
    "    # get the timestamps about the end of an experimental phase\n",
    "    sorted_exp_phase_timestamps = sorted(data[\"phaseFinishedTimestamps\"].items(),\n",
    "                                         key=operator.itemgetter(1))\n",
    "\n",
    "    # create a dictionary with the start and end timestamp of each experimental phase\n",
    "    phase_start_end_stamps = {sorted_exp_phase_timestamps[i + 1][0]: {\"Start\": sorted_exp_phase_timestamps[i][1],\n",
    "                                                                      \"End\": sorted_exp_phase_timestamps[i + 1][1]}\n",
    "                              for i in range(len(sorted_exp_phase_timestamps) - 1)}\n",
    "\n",
    "    # add additional custom experimental phases\n",
    "\n",
    "    # High-Stress & Low-Stress Phase (Beginning of first Mental Arithmetic Task in the High-Stress/Low-Stress\n",
    "    # Condition to the end of the last task in the High-Stress/Low-Stress Condition\n",
    "    phase_start_end_stamps[sorted_exp_phase_timestamps[12][0][:2] + \"_phase\"] = \\\n",
    "        {\"Start\": sorted_exp_phase_timestamps[12][1], \"End\": sorted_exp_phase_timestamps[26][1]}\n",
    "    phase_start_end_stamps[sorted_exp_phase_timestamps[31][0][:2] + \"_phase\"] = \\\n",
    "        {\"Start\": sorted_exp_phase_timestamps[31][1], \"End\": sorted_exp_phase_timestamps[45][1]}\n",
    "\n",
    "    # Mental Arithmetic Typing Task plus Typing Task\n",
    "    phase_start_end_stamps[\"HS_typing\"] = {\n",
    "        \"Start\": phase_start_end_stamps[\"HS_Mental_Arithmetic_PatternTyping\"][\"Start\"],\n",
    "        \"End\": phase_start_end_stamps[\"HS_PatternTyping\"][\"End\"]}\n",
    "    phase_start_end_stamps[\"LS_typing\"] = {\n",
    "        \"Start\": phase_start_end_stamps[\"LS_Mental_Arithmetic_PatternTyping\"][\"Start\"],\n",
    "        \"End\": phase_start_end_stamps[\"LS_PatternTyping\"][\"End\"]}\n",
    "\n",
    "    return phase_start_end_stamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Heart data processing functions ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot the rr timeseries (adopted the plot_timeseries function from the hrvanalysis package)\n",
    "def plot_rr_timeseries(nn_intervals: List[float], normalize: bool = True,\n",
    "                    autoscale: bool = True, y_min: float = None, y_max: float = None, plot_title: str = None):\n",
    "\n",
    "    \"\"\"\n",
    "        Function plotting the NN-intervals time series.\n",
    "        Arguments\n",
    "        ---------\n",
    "        nn_intervals : list\n",
    "            list of Normal to Normal Interval.\n",
    "        normalize : bool\n",
    "            Set to True to plot X axis as a cumulative sum of Time.\n",
    "            Set to False to plot X axis using x as index array 0, 1, ..., N-1.\n",
    "        autoscale : bool\n",
    "            Option to normalize the x-axis as a time series for comparison. Set to True by default.\n",
    "        y_min : float\n",
    "            Custom min value might be set for y axis.\n",
    "        y_max : float\n",
    "            Custom max value might be set for y axis.\n",
    "            :param y_min:\n",
    "            :param y_max:\n",
    "            :param plot_title:\n",
    "        \"\"\"\n",
    "\n",
    "    style.use(\"seaborn-darkgrid\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Rr Interval time series for \" + plot_title)\n",
    "    plt.ylabel(\"Rr Interval\", fontsize=15)\n",
    "\n",
    "    if normalize:\n",
    "        plt.xlabel(\"Time (s)\", fontsize=15)\n",
    "        plt.plot(np.cumsum(nn_intervals) / 1000, nn_intervals)\n",
    "    else:\n",
    "        plt.xlabel(\"RR-interval index\", fontsize=15)\n",
    "        plt.plot(nn_intervals)\n",
    "\n",
    "    if not autoscale:\n",
    "        plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calc_bpm(rr_intervals):\n",
    "\n",
    "    # remove 0 values\n",
    "    rr_intervals = [i for i in rr_intervals if i !=0]\n",
    "\n",
    "    # remove remaining outliers\n",
    "    rr_intervals = hrvanalysis.remove_outliers(rr_intervals, low_rri=350, high_rri=1800, verbose=False)\n",
    "\n",
    "    # interpolate outlier values\n",
    "    rr_intervals = hrvanalysis.interpolate_nan_values(rr_intervals, interpolation_method=\"linear\")\n",
    "\n",
    "    # remove NaN values, which might still be in the dataset (interpolation doesnt work on the first and last datapoint\n",
    "    # in the dataset\n",
    "    rr_intervals = [i for i in rr_intervals if not np.isnan(i)]\n",
    "\n",
    "    # calculate the average bpm from the cleaned rr data\n",
    "    mean_bpm = hrvanalysis.get_time_domain_features(rr_intervals)[\"mean_hr\"]\n",
    "\n",
    "    return mean_bpm\n",
    "\n",
    "\n",
    "# get and process the heart data (input is the firebase dataset of a single participant as well as the participant id)\n",
    "def process_hr_data(dataset, participant):\n",
    "\n",
    "    # Path to Heart Data\n",
    "    data_path = \"LabStudy_Heart_RawData/\"\n",
    "\n",
    "    hr_results = {}\n",
    "\n",
    "    # get the physio data id to be able to match the heart data with the firebase dara\n",
    "    physio_id = dataset[\"MetaData\"][\"physioDataId\"]\n",
    "\n",
    "    # get the participant hear rate data file\n",
    "    heart_data = pd.read_csv(data_path + physio_id + \".csv\", sep=\";\")\n",
    "\n",
    "    # Convert the UniversalTimestamps into an Epoch Timestamps (timestamp had a 2-hour offset with conversion\n",
    "    heart_data[\"convertedTimestamp\"] = [int(((i - 621355968000000000) / 10000)) for i in\n",
    "                                        heart_data[\"UniversalTimestamp\"]]\n",
    "\n",
    "    # Convert the RR-Interval String values into floats\n",
    "    heart_data[\"IBI\"] = [float(i.replace(\",\", \".\")) for i in heart_data[\"IBI\"]]\n",
    "\n",
    "    # get the timestamps to extract the relevant heart rate data for each experimental phase from the heart\n",
    "    # rate data set\n",
    "    phase_start_end_stamps = get_phase_timestamps(dataset)\n",
    "\n",
    "    # choose the experimental phases for which the HR data should be looked at\n",
    "    relevant_phases = [\"LS_Mental_Arithmetic_PatternTyping\", \"LS_PatternTyping\", \"LS_phase\", \"LS_typing\",\n",
    "                       \"HS_Mental_Arithmetic_PatternTyping\", \"HS_PatternTyping\", \"HS_phase\", \"HS_typing\"]\n",
    "\n",
    "    # --- get the relevant experimental phase heart rate data, inspect it and calculate the average BPM if\n",
    "    # there is the heart rate data is sufficient ---\n",
    "\n",
    "    for phase in relevant_phases:\n",
    "\n",
    "        phase_hr_data = heart_data.loc[\n",
    "            (phase_start_end_stamps[phase][\"Start\"] < heart_data[\"convertedTimestamp\"]) &\n",
    "            (heart_data[\"convertedTimestamp\"] < phase_start_end_stamps[phase][\"End\"])]\n",
    "\n",
    "        # Data Inspection\n",
    "\n",
    "        # Step 1: Plot the Data for each relevant Phase\n",
    "        # plot_rr_timeseries(phase_hr_data[\"IBI\"], plot_title=participant + \" in \" + phase)\n",
    "\n",
    "        # Step 2: get info about data quality and potential outlier values\n",
    "        # get the number of valid datapoints (valid, if they are in a range between 350 and 1800\n",
    "        val_datapoints = sum([1 for i in phase_hr_data[\"IBI\"] if 350 < i < 1800])\n",
    "        # get the percentage artefacts in the data (0 datapoints are excluded, because they will be removed from\n",
    "        # the dataset in a later step and might not represent measurement artifacts per se)\n",
    "        # have the if else here to prevent a division by 0 if there are no heart data points for the phase\n",
    "        if val_datapoints > 5:\n",
    "            artifact_percentage = 100 - ((val_datapoints + len(phase_hr_data[phase_hr_data[\"IBI\"] == 0])) /\n",
    "                                         len(phase_hr_data) * 100)\n",
    "        else:\n",
    "            artifact_percentage = 100\n",
    "\n",
    "        # Outlier Removal and Heart Rate Data Preprocessing\n",
    "        # ignore participants with less than 5 data points in a phase and more than 5% artifact percentage\n",
    "        if val_datapoints > 5 and artifact_percentage < 5:\n",
    "\n",
    "            bpm = calc_bpm(phase_hr_data[\"IBI\"])\n",
    "\n",
    "            hr_results[phase + \"_BPM\"] = bpm\n",
    "\n",
    "        else:\n",
    "            print(participant + \" has invalid heart data in phase \" + phase)\n",
    "\n",
    "    return hr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- EDA data processing functions ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot the eda data, similar to the hr data visualization\n",
    "def plot_eda_data(eda_data, plot_title):\n",
    "\n",
    "    style.use(\"seaborn-darkgrid\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"EDA time series for \" + plot_title)\n",
    "    plt.ylabel(\"EDA Value\", fontsize=15)\n",
    "\n",
    "    plt.xlabel(\"EDA index\", fontsize=15)\n",
    "    plt.plot(eda_data)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get and process the eda data (input is the firebase dataset of a single participant as well as the participant id)\n",
    "def process_eda_data(dataset, participant):\n",
    "\n",
    "    # Path to EDA Data\n",
    "    data_path = \"LabStudy_EDA_RawData/\"\n",
    "\n",
    "    eda_results = {}\n",
    "\n",
    "    # get the physio data id to be able to match the heart data with the firebase dara\n",
    "    physio_id = dataset[\"MetaData\"][\"physioDataId\"]\n",
    "\n",
    "    # Use the physioID to create the heart data file\n",
    "    eda_data = pd.read_csv(data_path + physio_id + \".csv\", sep=\";\")\n",
    "\n",
    "    # Convert the UniversalTimestamps into an Epoch Timestamps (timestamp had a 2-hour offset with conversion\n",
    "    # formular used in the other script - (120 *\n",
    "    eda_data[\"convertedTimestamp\"] = [int(((i - 621355968000000000) / 10000)) for i in\n",
    "                                      eda_data[\"UniversalTimestamp\"]]\n",
    "\n",
    "    # Convert the RR-Interval String values into floats\n",
    "    eda_data[\"EDA\"] = [float(i) for i in eda_data[\"EDA\"]]\n",
    "\n",
    "    # get the timestamps to extract the relevant eda data for each experimental phase from the eda data set\n",
    "    phase_start_end_stamps = get_phase_timestamps(dataset)\n",
    "\n",
    "    # choose the experimental phases for which the EDA data should be looked at\n",
    "    relevant_phases = [\"LS_Mental_Arithmetic_PatternTyping\", \"LS_PatternTyping\", \"LS_phase\", \"LS_typing\",\n",
    "                       \"HS_Mental_Arithmetic_PatternTyping\", \"HS_PatternTyping\", \"HS_phase\", \"HS_typing\"]\n",
    "\n",
    "    for phase in relevant_phases:\n",
    "        phase_eda_data = eda_data.loc[\n",
    "            (phase_start_end_stamps[phase][\"Start\"] < eda_data[\"convertedTimestamp\"]) &\n",
    "            (eda_data[\"convertedTimestamp\"] < phase_start_end_stamps[phase][\"End\"])]\n",
    "\n",
    "        # Data Inspection\n",
    "\n",
    "        # Step 1: Plot the eda data\n",
    "        # plot_eda_data(phase_eda_data[\"EDA\"], plot_title=participant + \" in \" + phase)\n",
    "\n",
    "        # Step 2: Check for potential artifacts (bad measurements)\n",
    "        if len(phase_eda_data[\"EDA\"]) > 5:\n",
    "            artifact_perc = 100 - (len(phase_eda_data[phase_eda_data[\"EDA\"] > 0]) /\n",
    "                                   len(phase_eda_data[\"EDA\"]) * 100)\n",
    "        else:\n",
    "            artifact_perc = 100\n",
    "\n",
    "        if artifact_perc < 5:\n",
    "\n",
    "            # apply a rolling median filter to the eda data to smoothen it\n",
    "            rolling_mean_eda = phase_eda_data[\"EDA\"].rolling(20).median().dropna()\n",
    "\n",
    "            # calculate the mean skin resistance of the phase\n",
    "            mean_eda = np.mean(rolling_mean_eda)\n",
    "\n",
    "            eda_results[phase + \"_EDA\"] = mean_eda\n",
    "\n",
    "        else:\n",
    "            print(participant + \" has invalid eda data in phase \" + phase)\n",
    "\n",
    "    return eda_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Self-Report data processing functions ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the self-report data (input is the firebase dataset of a single participant as well as the participant id)\n",
    "def process_self_report_data(dataset):\n",
    "\n",
    "    self_report_ratings = {}\n",
    "\n",
    "    # get a list of MDBF items that need to be recoded\n",
    "    items_to_recode = [\"MDBF_angespannt\", \"MDBF_nervÃ¶s\", \"MDBF_schlÃ¤frig\", \"MDBF_unglÃ¼cklich\", \"MDBF_unzufrieden\",\n",
    "                       \"MDBF_ermattet\"]\n",
    "\n",
    "    # get the MDBF items\n",
    "    for item in dataset[\"HS_Mdbf\"]:\n",
    "        if item in items_to_recode:\n",
    "            self_report_ratings[item + \"_HS\"] = 4 - dataset[\"HS_Mdbf\"][item]\n",
    "        else:\n",
    "            self_report_ratings[item + \"_HS\"] = dataset[\"HS_Mdbf\"][item]\n",
    "\n",
    "    for item in dataset[\"LS_Mdbf\"]:\n",
    "        if item in items_to_recode:\n",
    "            self_report_ratings[item + \"_LS\"] = 4 - dataset[\"LS_Mdbf\"][item]\n",
    "        else:\n",
    "            self_report_ratings[item + \"_LS\"] = dataset[\"LS_Mdbf\"][item]\n",
    "\n",
    "    # get the SAM items\n",
    "    for page in dataset:\n",
    "        if \"SAM\" in page:\n",
    "            for item in dataset[page]:\n",
    "                self_report_ratings[item + \"_\" + page] = dataset[page][item]\n",
    "\n",
    "    return self_report_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Process the heart data, eda data and self-report data for each participant and save it in one dataframe ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with the manipulation check variables (wrapper function to get the hr, eda and self-report data\n",
    "# and save it in a dataframe\n",
    "def create_man_check_dataset(dataset):\n",
    "\n",
    "    man_check_data = {}\n",
    "\n",
    "    for participant in dataset:\n",
    "\n",
    "        # if it is a real participant (if the participant finished the study)\n",
    "        if \"phaseFinishedTimestamps\" in dataset[participant] and \"BfiNeuroticism\" in \\\n",
    "                dataset[participant][\"phaseFinishedTimestamps\"]:\n",
    "\n",
    "            man_check_data[participant] = {}\n",
    "\n",
    "            print(\"Processing participant \" + participant)\n",
    "\n",
    "            # get the participant dataset\n",
    "            par_data = dataset[participant]\n",
    "\n",
    "            # get the manipulation check parameters\n",
    "            hr_params = process_hr_data(par_data, participant)\n",
    "            eda_params = process_eda_data(par_data, participant)\n",
    "            self_report_items = process_self_report_data(par_data)\n",
    "\n",
    "            # merge them into a single dictionary\n",
    "            man_check_data[participant] = {**hr_params, **eda_params, **self_report_items}\n",
    "\n",
    "    return man_check_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the manipulation check data dictionary\n",
    "man_check_data = create_man_check_dataset(firebase_data)\n",
    "\n",
    "# Transform the manipulation check data dic into a pandas dataframe and drop rows without data\n",
    "man_check_df = pd.DataFrame(man_check_data).T.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Manipulation check procedure ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: process mdbf scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the MDBF scales\n",
    "man_check_df[\"MDBF_GS_HS\"] = (man_check_df[\"MDBF_wohl_HS\"] + man_check_df[\"MDBF_gut_HS\"] + man_check_df[\n",
    "    \"MDBF_unglÃ¼cklich_HS\"] + man_check_df[\"MDBF_unzufrieden_HS\"]) / 4\n",
    "man_check_df[\"MDBF_RU_HS\"] = (man_check_df[\"MDBF_ausgeglichen_HS\"] + man_check_df[\"MDBF_ruhig_HS\"] + man_check_df[\n",
    "    \"MDBF_angespannt_HS\"] + man_check_df[\"MDBF_nervÃ¶s_HS\"]) / 4\n",
    "man_check_df[\"MDBF_WM_HS\"] = (man_check_df[\"MDBF_frisch_HS\"] + man_check_df[\"MDBF_wach_HS\"] + man_check_df[\n",
    "    \"MDBF_schlÃ¤frig_HS\"] + man_check_df[\"MDBF_ermattet_HS\"]) / 4\n",
    "\n",
    "man_check_df[\"MDBF_GS_LS\"] = (man_check_df[\"MDBF_wohl_LS\"] + man_check_df[\"MDBF_gut_LS\"] + man_check_df[\n",
    "    \"MDBF_unglÃ¼cklich_LS\"] + man_check_df[\"MDBF_unzufrieden_LS\"]) / 4\n",
    "man_check_df[\"MDBF_RU_LS\"] = (man_check_df[\"MDBF_ausgeglichen_LS\"] + man_check_df[\"MDBF_ruhig_LS\"] + man_check_df[\n",
    "    \"MDBF_angespannt_LS\"] + man_check_df[\"MDBF_nervÃ¶s_LS\"]) / 4\n",
    "man_check_df[\"MDBF_WM_LS\"] = (man_check_df[\"MDBF_frisch_LS\"] + man_check_df[\"MDBF_wach_LS\"] + man_check_df[\n",
    "    \"MDBF_schlÃ¤frig_LS\"] + man_check_df[\"MDBF_ermattet_LS\"]) / 4\n",
    "\n",
    "# calc Cronbach Alpha's of the scales\n",
    "cronbachs_alphas = {}\n",
    "\n",
    "mdbf_scales = {\n",
    "    \"GS_HS\": [\"MDBF_wohl_HS\", \"MDBF_gut_HS\", \"MDBF_unglÃ¼cklich_HS\", \"MDBF_unzufrieden_HS\"],\n",
    "    \"RU_HS\": [\"MDBF_ausgeglichen_HS\", \"MDBF_ruhig_HS\", \"MDBF_angespannt_HS\", \"MDBF_nervÃ¶s_HS\"],\n",
    "    \"WM_HS\": [\"MDBF_frisch_HS\", \"MDBF_wach_HS\", \"MDBF_schlÃ¤frig_HS\", \"MDBF_ermattet_HS\"],\n",
    "    \"GS_LS\": [\"MDBF_wohl_LS\", \"MDBF_gut_LS\", \"MDBF_unglÃ¼cklich_LS\", \"MDBF_unzufrieden_LS\"],\n",
    "    \"RU_LS\": [\"MDBF_ausgeglichen_LS\", \"MDBF_ruhig_LS\", \"MDBF_angespannt_LS\", \"MDBF_nervÃ¶s_LS\"],\n",
    "    \"WM_LS\": [\"MDBF_frisch_LS\", \"MDBF_wach_LS\", \"MDBF_schlÃ¤frig_LS\", \"MDBF_ermattet_LS\"]\n",
    "}\n",
    "\n",
    "for i in mdbf_scales:\n",
    "    cronbachs_alphas[i] = {}\n",
    "    alpha = pg.cronbach_alpha(data=man_check_df.loc[:, mdbf_scales[i]])\n",
    "    print(\"Cronbachs Alpha \" + str(i), alpha)\n",
    "    cronbachs_alphas[i][\"alpha\"] = alpha[0]\n",
    "    cronbachs_alphas[i][\"alpha_ci\"] = alpha[1]\n",
    "\n",
    "# save the cronbach alphas as a dataframe\n",
    "cronbachs_alphas_df = pd.DataFrame(cronbachs_alphas).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: get some additional SAM values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc a merged SAM and Arousal scale\n",
    "\n",
    "man_check_df[\"merged_Valence_HS\"] = (man_check_df[\"samValence_HS_SAM_DragDrop\"] +\n",
    "                                     man_check_df[\"samValence_HS_SAM_Drawing\"] +\n",
    "                                     man_check_df[\"samValence_HS_SAM_FollowBox\"] +\n",
    "                                     man_check_df[\"samValence_HS_SAM_PatternTyping\"] +\n",
    "                                     man_check_df[\"samValence_HS_SAM_PointClick\"]) / 5\n",
    "\n",
    "man_check_df[\"merged_Arousal_HS\"] = (man_check_df[\"samArousal_HS_SAM_DragDrop\"] +\n",
    "                                     man_check_df[\"samArousal_HS_SAM_Drawing\"] +\n",
    "                                     man_check_df[\"samArousal_HS_SAM_FollowBox\"] +\n",
    "                                     man_check_df[\"samArousal_HS_SAM_PatternTyping\"] +\n",
    "                                     man_check_df[\"samArousal_HS_SAM_PointClick\"]) / 5\n",
    "\n",
    "man_check_df[\"merged_Valence_LS\"] = (man_check_df[\"samValence_LS_SAM_DragDrop\"] +\n",
    "                                     man_check_df[\"samValence_LS_SAM_Drawing\"] +\n",
    "                                     man_check_df[\"samValence_LS_SAM_FollowBox\"] +\n",
    "                                     man_check_df[\"samValence_LS_SAM_PatternTyping\"] +\n",
    "                                     man_check_df[\"samValence_LS_SAM_PointClick\"]) / 5\n",
    "\n",
    "man_check_df[\"merged_Arousal_LS\"] = (man_check_df[\"samArousal_LS_SAM_DragDrop\"] +\n",
    "                                     man_check_df[\"samArousal_LS_SAM_Drawing\"] +\n",
    "                                     man_check_df[\"samArousal_LS_SAM_FollowBox\"] +\n",
    "                                     man_check_df[\"samArousal_LS_SAM_PatternTyping\"] +\n",
    "                                     man_check_df[\"samArousal_LS_SAM_PointClick\"]) / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: get descriptive stats of the dataset for the selected set of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_stats_df = man_check_df.describe().sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: perform a paired sample t-test on all relevant manipulation check items to compare them between the high-stress and low-stress condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitialize an empty dataframe to store the results of the paired sample t-test for each variable\n",
    "man_check_results = pd.DataFrame()\n",
    "\n",
    "# loop over the variable columns\n",
    "for col in man_check_df.columns:\n",
    "    if \"HS\" in col:\n",
    "        # find the corresponding low_stress variable\n",
    "        ls_string = col.replace(\"HS\", \"LS\")\n",
    "        # create a neutral variable name\n",
    "        neutral_string = col.replace(\"HS\", \"\")\n",
    "        # perform the paired sample t_test\n",
    "        paired_ttest = pg.ttest(man_check_df[col], man_check_df[ls_string], paired=True)\n",
    "        # rename the pingouin index of the t-test dataframe to the variable name\n",
    "        paired_ttest.index = [neutral_string]\n",
    "        # concat the results of the variables t-test to the result dataframe\n",
    "        man_check_results = pd.concat([man_check_results, paired_ttest])\n",
    "\n",
    "        # Plot the distribution of both variable pairs\n",
    "        sns.distplot(man_check_df[col], hist=True, kde=True, kde_kws={\"linewidth\": 3}, label=\"HS\")\n",
    "        sns.distplot(man_check_df[ls_string], hist=True, kde=True, kde_kws={\"linewidth\": 3}, label=\"LS\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(neutral_string)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save all manipulation check data in an excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe plus the descriptive stats as an excel file\n",
    "with pd.ExcelWriter(\"Labstudy_Manipulation_Check_Results.xlsx\") as writer:\n",
    "    man_check_results.to_excel(writer, float_format=\"%.4f\", sheet_name=\"Paired sample t-test results\")\n",
    "    descriptive_stats_df.to_excel(writer, float_format=\"%.3f\", sheet_name=\"Descriptive Stats\")\n",
    "    cronbachs_alphas_df.to_excel(writer, float_format=\"%.4f\", sheet_name=\"Cronbachs_Alphas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- In addition to the manipulation check, create a dataset with the manipulation check items for the machine learning analysis. The data format needs to match the keyboard feature machine learning data format ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the relevant manipulation check items for the machine learning dataset (to predict valence, arousal, bpm and eda\n",
    "# from the keyboard typing data)\n",
    "\n",
    "# The data needs to be structured in long format --> Each condition is a new column (2 columns per participant)\n",
    "# code is not the cleanest: The best approach would be to change the create_man_check_dataset to support the creation\n",
    "# of both long and wide data (this code was added in a later step and my shortest solution to \"fix\" the problem)\n",
    "man_check_data_long = {}\n",
    "for par in man_check_data:\n",
    "    # check if the physiological variables are in the dataset\n",
    "    if \"HS_PatternTyping_BPM\" in man_check_data[par]:\n",
    "        hs_bpm = man_check_data[par][\"HS_PatternTyping_BPM\"]\n",
    "    else:\n",
    "        hs_bpm = None\n",
    "\n",
    "    if \"HS_PatternTyping_EDA\" in man_check_data[par]:\n",
    "        hs_eda = man_check_data[par][\"HS_PatternTyping_EDA\"]\n",
    "    else:\n",
    "        hs_eda = None\n",
    "\n",
    "    if \"LS_PatternTyping_BPM\" in man_check_data[par]:\n",
    "        ls_bpm = man_check_data[par][\"LS_PatternTyping_BPM\"]\n",
    "    else:\n",
    "        ls_bpm = None\n",
    "\n",
    "    if \"LS_PatternTyping_EDA\" in man_check_data[par]:\n",
    "        ls_eda = man_check_data[par][\"LS_PatternTyping_EDA\"]\n",
    "    else:\n",
    "        ls_eda = None\n",
    "\n",
    "    man_check_data_long[\"HS_\" + par] = {\"BPM\": hs_bpm,\n",
    "                                        \"EDA\": hs_eda,\n",
    "                                        \"Valence\": man_check_data[par][\"samArousal_HS_SAM_PatternTyping\"],\n",
    "                                        \"Arousal\": man_check_data[par][\"samValence_HS_SAM_PatternTyping\"]\n",
    "                                        }\n",
    "    man_check_data_long[\"LS_\" + par] = {\"BPM\": ls_bpm,\n",
    "                                        \"EDA\": ls_eda,\n",
    "                                        \"Valence\": man_check_data[par][\"samArousal_LS_SAM_PatternTyping\"],\n",
    "                                        \"Arousal\": man_check_data[par][\"samValence_LS_SAM_PatternTyping\"]\n",
    "                                        }\n",
    "\n",
    "man_check_data_long = pd.DataFrame(man_check_data_long).T.dropna(how=\"all\")\n",
    "man_check_data_long.to_csv(\"Lab_Study_Manipulation_Check_Variables_for_ML.csv\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:full_conda]",
   "language": "python",
   "name": "conda-env-full_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
