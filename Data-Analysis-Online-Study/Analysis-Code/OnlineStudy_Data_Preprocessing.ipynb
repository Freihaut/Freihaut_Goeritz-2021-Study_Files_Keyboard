{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File to preprocess the data of the online-study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset import\n",
    "\n",
    "# open the dataset\n",
    "with open(\"OnlineStudy_RawData.json\") as jsonData:\n",
    "    dataset = json.load(jsonData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- get a dataset with the relevant Study MetaData Info ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaData = {}\n",
    "\n",
    "for participant in dataset:\n",
    "\n",
    "    metaData[participant] = {}\n",
    "\n",
    "    ignore = [\"taskOrder\", \"condition\", \"initScreenProps\"]\n",
    "\n",
    "    for i in dataset[participant][\"ExperimentMetaData\"]:\n",
    "        if i not in ignore and \"failsMediaCheck\" not in i:\n",
    "            metaData[participant][i] = dataset[participant][\"ExperimentMetaData\"][i]\n",
    "\n",
    "    if \"DonationOption\" in dataset[participant]:\n",
    "        metaData[participant][\"isDonating\"] = dataset[participant][\"DonationOption\"][\"data\"][\"isDonating\"]\n",
    "\n",
    "    if \"Soziodem\" in dataset[participant]:\n",
    "        for i in dataset[participant][\"Soziodem\"][\"data\"]:\n",
    "            metaData[participant][i] = dataset[participant][\"Soziodem\"][\"data\"][i]\n",
    "\n",
    "df = pd.DataFrame(metaData).T\n",
    "\n",
    "# create a \"complete study completed column\"\n",
    "df.loc[(df[\"hasCompleted\"] == True) | (df[\"lastCompletedPage\"] == \"DonationOption\")\n",
    "       | (df[\"lastCompletedPage\"] == \"Con_Mdbf\"), \"studyCompleted\"] = True\n",
    "df[\"studyCompleted\"].fillna(False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log some metadata information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of study accesses\n",
    "tot_study_open = len(df)\n",
    "# total number of study accesses without a valid panel ID\n",
    "no_panel_id = len(df[df[\"panelId\"] == \"undefined\"])\n",
    "# total number of study accesses with a valid panel ID\n",
    "valid_panel_id = tot_study_open - no_panel_id\n",
    "# internet explorer openings\n",
    "used_ie = len(df[df[\"isInternetExplorer\"]])\n",
    "# edge openings\n",
    "used_edge = len(df[df[\"isEdge\"]])\n",
    "# completed study with edge\n",
    "edge_compl = len(df[df[\"isEdge\"] & df[\"studyCompleted\"]])\n",
    "# fails the initial media check (touch or screen too small)\n",
    "fails_init_media_check = len(df[df[\"failsInitialMediaCheck\"] == True])\n",
    "# answer that no mouse is used in the study\n",
    "no_mouse = len(df[df[\"hasNoMouse\"] == True])\n",
    "# completed the study\n",
    "completed = len(df[df[\"studyCompleted\"] == True])\n",
    "\n",
    "# print some information about the dataset\n",
    "print(\"Total number of study opens: \", tot_study_open)\n",
    "print(\"Study opens without a panel Id: \", no_panel_id)\n",
    "print(\"Study opens with a panel Id: \", valid_panel_id)\n",
    "print(\"Used Internet Explorer to open the study: \", used_ie)\n",
    "print(\"Used Edge to open the study: \", used_edge)\n",
    "print(\"Finished the study with Edge: \", edge_compl)\n",
    "print(\"Fails initial Media check: \", fails_init_media_check)\n",
    "print(\"Reports using no mouse: \", no_mouse)\n",
    "print(\"Finished the study:\", completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Create a dataset with participants who finished the study and exclude the data of participants who finished the study more than once ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the number of times persons with the same panel id participated\n",
    "panel_ids = df[\"panelId\"].value_counts()\n",
    "repetitive_panel_ids = panel_ids[panel_ids > 1]\n",
    "print(\"Number of Participants who opened the study more than once: \", len(repetitive_panel_ids))\n",
    "\n",
    "# create a df that only has unique panel ids\n",
    "cleaned_df = df\n",
    "\n",
    "# loop over the repetive panel Ids\n",
    "for panel_id in repetitive_panel_ids.index:\n",
    "\n",
    "    # get the data of the person that opened the study more than once\n",
    "    pers = cleaned_df[cleaned_df[\"panelId\"] == panel_id]\n",
    "\n",
    "    # get the index of the completed trials\n",
    "    index_to_keep = pers[pers[\"studyCompleted\"] == True].index\n",
    "    # check is the same person completed the study more than once\n",
    "    have_completed = pers[pers[\"studyCompleted\"] == True]\n",
    "    # if the study was completed more than once by the same person, only keep the first data of the first completion\n",
    "    if len(have_completed) > 1:\n",
    "        print(\"Participant with panel id \" + panel_id + \" completed the study \" + str(len(have_completed)) + \" times\")\n",
    "        # only keep the first completed trial\n",
    "        index_to_keep = pd.to_numeric(have_completed[\"startTime\"]).idxmin()\n",
    "\n",
    "    # if the study was completed, keep the trials\n",
    "    if len(index_to_keep) > 0:\n",
    "        ind_to_del = pers.index.drop(index_to_keep)\n",
    "    # if the study was not completed, keep one\n",
    "    else:\n",
    "        ind_to_del = pers.index[:-1]\n",
    "\n",
    "    cleaned_df = cleaned_df.drop(ind_to_del)\n",
    "\n",
    "# check if repetivive ids have been successfully removed\n",
    "new_panel_ids = cleaned_df[\"panelId\"].value_counts()\n",
    "new_repetitive_panel_ids = new_panel_ids[new_panel_ids > 1]\n",
    "print(\"Number of Participants who opened the study more than once after cleanup: \", len(new_repetitive_panel_ids))\n",
    "\n",
    "# remove the \"undefined\" --> be careful if there are completed trials with an undefined panel ID\n",
    "# indicates that someone messed around with the URL parameter (was not the case in the dataset)\n",
    "cleaned_df = cleaned_df.drop(cleaned_df[cleaned_df[\"panelId\"] == \"undefined\"].index)\n",
    "\n",
    "print(\"Unique study access: \", len(cleaned_df))\n",
    "print(\"Number of unique study finishes: \", len(cleaned_df[cleaned_df[\"studyCompleted\"]]))\n",
    "\n",
    "# save the index values (firebase ids) of the completed studies\n",
    "ids_completed = cleaned_df[cleaned_df[\"studyCompleted\"]].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Outlier and bad case removal --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal: detect participants who did not properly work on the study (and therefore distort the study results)\n",
    "\n",
    " strategies to detect bad cases:\n",
    " - Timing based detection (clicking through or taking non-desired breaks)\n",
    " - Answer based detection (no variance in answers or answers are given randomly)\n",
    " - Technical difficulties in the count task\n",
    " - No compliance in the count task (no answers given, random answers given, no variance in answers)\n",
    " - No compliance in the typing task (tasks instructions are not followed) --> is checked in a later step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create a dataframe with the study duration and study page durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_durations = {}\n",
    "\n",
    "# get the duration of the task in the practice condition and ignore the duration of the instruction and demo trial\n",
    "pr_tasks = [\"Pr_DragDrop\", \"Pr_FollowBox\", \"Pr_PatternTyping\", \"Pr_PointClick\", \"Pr_Slider\"]\n",
    "\n",
    "# loop over all datasets\n",
    "for par in dataset:\n",
    "    # only include the data of the finished studies\n",
    "    if par in ids_completed:\n",
    "\n",
    "        study_durations[par] = {}\n",
    "\n",
    "        # get info about the page duration and the study duration\n",
    "        study_duration = 0\n",
    "        condition_duration = 0\n",
    "\n",
    "        # loop over all study pages and save the page duration time plus add it to the total time\n",
    "        for study_page in dataset[par]:\n",
    "\n",
    "            if \"MetaData\" in dataset[par][study_page]:\n",
    "                study_durations[par][study_page + \"_duration\"] = dataset[par][study_page][\"MetaData\"][\n",
    "                                                                       \"pageDuration\"] / 1000\n",
    "                study_duration += dataset[par][study_page][\"MetaData\"][\"pageDuration\"]\n",
    "\n",
    "                # if its a practice task, additionally get the duration of the task and ignore the duration of the\n",
    "                # instruction and demo trial\n",
    "                if study_page in pr_tasks:\n",
    "                    study_durations[par][study_page + \"_Task_duration\"] = (dataset[par][study_page][\"data\"][\n",
    "                                                                         \"taskEnded\"] - dataset[par][study_page][\"data\"][\n",
    "                                                                         \"trialStarted\"]) / 1000\n",
    "                # if its a page from the actual condition, add its time to the condition duration (except for the intro\n",
    "                # page)\n",
    "                if \"Con_\" in study_page and study_page != \"Con_Instr\":\n",
    "                    condition_duration += dataset[par][study_page][\"MetaData\"][\"pageDuration\"]\n",
    "\n",
    "        # save total study duration in minutes\n",
    "        study_durations[par][\"study_duration\"] = study_duration / 1000 / 60\n",
    "        # save the condition duration\n",
    "        study_durations[par][\"condition_duration\"] = condition_duration / 1000 / 60\n",
    "\n",
    "study_durations_df = pd.DataFrame(study_durations).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  --- 1. Timing based bad case detection ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag cases that:\n",
    "\n",
    "# - had a total study duration shorter than a possible time for actual participation (personal time to click through\n",
    "# the study without reading anything and with knowledge about the study was about 12 minutes)\n",
    "\n",
    "short_study_duration = study_durations_df.loc[study_durations_df[\"study_duration\"] < 12]\n",
    "print(\"Short study duration outliers\", len(short_study_duration))\n",
    "\n",
    "# - had a too short duration to fill out the mdbf (in personal tests, clicking through the mdbf without reading\n",
    "# took about 13-15 seconds), filling the questionnaire out as fast as possible with \"correct\" answers took about 20\n",
    "# seconds (with good knowledge about the questions)\n",
    "\n",
    "# the mdbf in the practice condition should take longer than the mdbf in the actual condition because it is new\n",
    "short_mdbf_pr = study_durations_df.loc[study_durations_df[\"Pr_Mdbf_duration\"] < 21]\n",
    "short_mdbf_co = study_durations_df.loc[study_durations_df[\"Con_Mdbf_duration\"] < 18]\n",
    "print(\"Co_MDBF duration outliers condition\", len(short_mdbf_co),\n",
    "      \"\\n\" + \"Pr_MDBF duration outliers practice\", len(short_mdbf_pr))\n",
    "\n",
    "# the count task on average took longer than 65 seconds in any count task (either because of technical difficulties or\n",
    "# because the task was interrupted by changing browser tabs etc... (-> the manipulation did not work as intended!)\n",
    "# the animation of the loading bar had problems \"loading\" in some cases, which caused the loading task length to be\n",
    "# pretty high\n",
    "# the expected count task time was 45 seconds\n",
    "# This was included because technical difficulties (or pauses during the task) likely changed the outcome of the\n",
    "# manipulation in a not desired direction --> technical difficulties cause frustration and stress independent of the\n",
    "# condition\n",
    "\n",
    "study_durations_df[\"avg_count_task_duration\"] = study_durations_df.loc[:, [\"Con_Count_DragDrop_duration\",\n",
    "                                                                           \"Con_Count_FollowBox_duration\",\n",
    "                                                                           \"Con_Count_PatternTyping_duration\",\n",
    "                                                                           \"Con_Count_PointClick_duration\",\n",
    "                                                                           \"Con_Count_Slider_duration\"]].mean(axis=1)\n",
    "\n",
    "bad_count_duration = study_durations_df.loc[study_durations_df[\"avg_count_task_duration\"] > 65]\n",
    "\n",
    "print(\"Average Count task took too long\", len(bad_count_duration))\n",
    "\n",
    "# Additionally filter out participant with a bad count task duration in the typing task\n",
    "bad_count_duration_typing = study_durations_df.loc[study_durations_df[\"Con_Count_PatternTyping_duration\"] > 65]\n",
    "\n",
    "print(\"Typing task count task took too long\", len(bad_count_duration_typing))\n",
    "\n",
    "\n",
    "# Add all time based outliers together and print info about how many need to be removed from the dataset\n",
    "timebased_outliers = list(short_study_duration.index) + list(short_mdbf_pr.index) + list(short_mdbf_co.index) + \\\n",
    "                     list(bad_count_duration.index) + list(bad_count_duration_typing.index)\n",
    "\n",
    "print(\"Total time based outliers\", len(list(set(timebased_outliers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- 2. Questionnaire answers based bad case detection ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cases without variance in the mdbf questionnaires (participants always clicked on the same answer in the MDBF)\n",
    "\n",
    "# create a df with the mdbf answers\n",
    "mdbf_answers = {}\n",
    "\n",
    "for par in dataset:\n",
    "    # only include the data of the finished studies\n",
    "    if par in ids_completed:\n",
    "\n",
    "        mdbf_answers[par] = {}\n",
    "\n",
    "        # Condition\n",
    "        for item in dataset[par][\"Con_Mdbf\"][\"data\"]:\n",
    "            value = dataset[par][\"Con_Mdbf\"][\"data\"][item]\n",
    "            mdbf_answers[par][\"Con_\" + item] = value\n",
    "\n",
    "        # Practice\n",
    "        for item in dataset[par][\"Pr_Mdbf\"][\"data\"]:\n",
    "            value = dataset[par][\"Pr_Mdbf\"][\"data\"][item]\n",
    "            mdbf_answers[par][\"Pr_\" + item] = value\n",
    "\n",
    "mdbf_answers_df = pd.DataFrame(mdbf_answers).T\n",
    "\n",
    "# filter the practice and actual condition, calculate the standard deviation of the mdbf answers and get the cases\n",
    "# that have no standard deviation (= answered all questions equally)\n",
    "practice_mdfb = mdbf_answers_df.filter(regex=\"Pr\")\n",
    "practice_mdfb = practice_mdfb.assign(std=practice_mdfb.std(axis=1))\n",
    "no_variance_pr = practice_mdfb.loc[practice_mdfb[\"std\"] == 0]\n",
    "\n",
    "print(\"No variance in the practice mdbf\", len(no_variance_pr))\n",
    "\n",
    "condition_mdfb = mdbf_answers_df.filter(regex=\"Con\")\n",
    "condition_mdfb = condition_mdfb.assign(std=condition_mdfb.std(axis=1))\n",
    "no_variance_con = condition_mdfb.loc[condition_mdfb[\"std\"] == 0]\n",
    "\n",
    "print(\"No variance in the practice mdbf\", len(no_variance_con))\n",
    "\n",
    "bad_mdbf_answers = list(no_variance_pr.index) + list(no_variance_con.index)\n",
    "# filter double cases\n",
    "bad_mdbf_answers = list(set(bad_mdbf_answers))\n",
    "print(\"Total number of bad mdbf answer cases\", len(bad_mdbf_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- 3. Stress manipulation task (count task) answers based bad case detection ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cases with bad answer behavior in the count task (stress manipulation was not done properly)\n",
    "\n",
    "# get the count task answers\n",
    "\n",
    "count_tasks = ['Con_CountAns_DragDrop',\n",
    "               'Con_CountAns_FollowBox',\n",
    "               'Con_CountAns_PatternTyping',\n",
    "               'Con_CountAns_PointClick',\n",
    "               'Con_CountAns_Slider']\n",
    "\n",
    "count_answers_data = {}\n",
    "\n",
    "for par in dataset:\n",
    "\n",
    "    if par in ids_completed:\n",
    "\n",
    "        count_answers_data[par] = {}\n",
    "\n",
    "        # get condition\n",
    "        count_answers_data[par][\"condition\"] = dataset[par][\"ExperimentMetaData\"][\"condition\"]\n",
    "\n",
    "        for i in count_tasks:\n",
    "            # get the string of the count task (to get the correct target number)\n",
    "            k = i.replace(\"CountAns\", \"Count\")\n",
    "\n",
    "            task_string = i[13:]\n",
    "\n",
    "            count_task_answer = dataset[par][i][\"data\"][\"Count_Task_Answer\"]\n",
    "            count_task_solution = dataset[par][k][\"data\"][\"Total_num_targets\"]\n",
    "\n",
    "            # get the answer and solution of the count task aswell as the difference between the answer and solution\n",
    "            # per task\n",
    "\n",
    "            count_answers_data[par][task_string + \"_CountSol\"] = count_task_solution\n",
    "            count_answers_data[par][task_string + \"_CountAns\"] = count_task_answer\n",
    "            count_answers_data[par][task_string + \"Difference\"] = abs(count_task_solution - count_task_answer)\n",
    "\n",
    "count_df = pd.DataFrame(count_answers_data).T\n",
    "\n",
    "# add columns about the total count task results (total solution, total answer, total difference between all\n",
    "# answers and all solutions aswell as the difference between the final answer and the final solution\n",
    "count_df[\"total_solution\"] = count_df.loc[: , count_df.columns.str.contains(\"_CountSol\")].sum(axis=1)\n",
    "count_df[\"total_answer\"] = count_df.loc[: , count_df.columns.str.contains(\"_CountAns\")].sum(axis=1)\n",
    "count_df[\"total_difference\"] = count_df.loc[: , count_df.columns.str.contains(\"Difference\")].sum(axis=1)\n",
    "count_df[\"result_difference\"] = abs(count_df[\"total_solution\"] - count_df[\"total_answer\"])\n",
    "\n",
    "# separate the high stress and low stress condition\n",
    "hs_count_df = count_df.loc[count_df[\"condition\"] == 0]\n",
    "ls_count_df = count_df.loc[count_df[\"condition\"] == 1]\n",
    "\n",
    "# get cases that always give the same answers in all count tasks: likely did not do the task properly\n",
    "hs_count_answers = hs_count_df.filter(regex=\"CountAns\")\n",
    "hs_count_answers = hs_count_answers.assign(std=hs_count_answers.std(axis=1))\n",
    "no_variance_count_hs = hs_count_answers.loc[hs_count_answers[\"std\"] == 0]\n",
    "\n",
    "ls_count_answers = ls_count_df.filter(regex=\"CountAns\")\n",
    "ls_count_answers = ls_count_answers.assign(std=ls_count_answers.std(axis=1))\n",
    "no_variance_count_ls = ls_count_answers.loc[ls_count_answers[\"std\"] == 0]\n",
    "\n",
    "\n",
    "bad_cases_count = list(no_variance_count_hs.index) + \\\n",
    "                  list(no_variance_count_ls.index)\n",
    "bad_cases_count = list(set(bad_cases_count))\n",
    "print(\"Total bad cases in the count task\", len(bad_cases_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Bring bad cases together and save which participants have valid data for further analysis ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all possible bad datapoints based on the different analysis and save it as a pickle file\n",
    "total_bad_cases = timebased_outliers + bad_mdbf_answers + bad_cases_count\n",
    "# filter duplicate cases\n",
    "total_bad_cases = list(set(total_bad_cases))\n",
    "\n",
    "print(\"Total bad cases identified with the previous steps\", len(total_bad_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the study duration\n",
    "new_study_durations = study_durations_df.drop(total_bad_cases)\n",
    "\n",
    "print(\"Participants after bad case removal\", len(new_study_durations))\n",
    "print(\"Median Study duration:\", np.median(new_study_durations[\"study_duration\"]))\n",
    "print(\"Standard Deviation of Study duration:\", np.std(new_study_durations[\"study_duration\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the index values of all participants who completed the study and the index values of the participants who\n",
    "# completed the study after removing the bad cases\n",
    "\n",
    "# study ids after removal of bad cases\n",
    "with open(\"filtered_online_study_ids_studyLevel\", \"wb\") as fp:\n",
    "    pickle.dump(set(ids_completed) - set(total_bad_cases), fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:full_conda]",
   "language": "python",
   "name": "conda-env-full_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
